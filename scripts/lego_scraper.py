import requests
from bs4 import BeautifulSoup
import json
import re
import time
from urllib.parse import urljoin
import sys
import random

class LegoScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.product_counter = 10
        
    def extract_product_data(self, url):
        """Extrai dados de um produto especÃ­fico"""
        try:
            print(f"Extraindo dados de: {url}")
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extrair nome do produto
            name = self.extract_name(soup)
            
            # Extrair preÃ§os
            price = self.extract_price(soup)
            
            # Extrair imagens
            images = self.extract_images(soup, url)
            
            # Extrair descriÃ§Ã£o (limitada a 6 linhas)
            description = self.extract_description(soup)
            
            # Extrair nÃºmero de peÃ§as
            pieces = self.extract_pieces(soup)
            
            # Extrair idade recomendada
            age = self.extract_age(soup)
            
            item_number = self.extract_item_number(url)
            
            vip_points = self.extract_vip_points(soup, price)
            
            rating = round(random.uniform(4.0, 5.0), 1)
            reviews = random.randint(50, 500)
            
            product_data = {
                "id": str(self.product_counter),
                "name": name,
                "price": price,
                "originalPrice": None,  # Sempre null
                "rating": rating,
                "reviews": reviews,
                "ages": age,
                "pieces": pieces,
                "itemNumber": item_number,
                "vipPoints": vip_points,
                "images": images,
                "description": description,
                "features": [],  # Array vazio conforme solicitado
                "inStock": True,  # Sempre em estoque
                "puzzleImage": images[0] if images else "/placeholder.svg?height=400&width=400&text=LEGO+Product",  # Primeira imagem como puzzle
                "puzzleTimeLimit": 300,  # Tempo limite do puzzle
                "puzzleDiscount": 70     # Desconto padrÃ£o do puzzle
            }
            
            self.product_counter += 1
            
            print(f"âœ“ Produto extraÃ­do: {name}")
            return product_data
            
        except Exception as e:
            print(f"âœ— Erro ao extrair {url}: {str(e)}")
            return None
    
    def extract_name(self, soup):
        """Extrai o nome do produto"""
        selectors = [
            'h1.vtex-store-components-3-x-productNameContainer',
            'h1[data-testid="product-name"]',
            '.product-name h1',
            'h1.product-title',
            'h1'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return "Produto LEGO"
    
    def extract_price(self, soup):
        """Extrai o preÃ§o do produto"""
        selectors = [
            '.vtex-product-price-1-x-sellingPrice',
            '.vtex-store-components-3-x-currencyContainer',
            '.price-selling',
            '.product-price .selling-price',
            '[data-testid="price-selling"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text().strip()
                # Extrair apenas nÃºmeros e vÃ­rgula/ponto
                price_match = re.search(r'[\d.,]+', price_text.replace('R$', '').replace(' ', ''))
                if price_match:
                    price_str = price_match.group().replace('.', '').replace(',', '.')
                    try:
                        return float(price_str)
                    except:
                        continue
        
        return 299.99  # PreÃ§o padrÃ£o
    
    def extract_original_price(self, soup, current_price):
        """Extrai o preÃ§o original (sem desconto)"""
        selectors = [
            '.vtex-product-price-1-x-listPrice',
            '.price-list',
            '.original-price',
            '[data-testid="price-list"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text().strip()
                price_match = re.search(r'[\d.,]+', price_text.replace('R$', '').replace(' ', ''))
                if price_match:
                    price_str = price_match.group().replace('.', '').replace(',', '.')
                    try:
                        original = float(price_str)
                        if original > current_price:
                            return original
                    except:
                        continue
        
        # Se nÃ£o encontrar preÃ§o original, calcular baseado no preÃ§o atual
        return round(current_price * 1.2, 2)  # 20% a mais que o preÃ§o atual

    def extract_images(self, soup, base_url):
        """Extrai URLs das imagens do produto"""
        images = []
        
        # Seletores para imagens
        selectors = [
            '.vtex-store-components-3-x-productImageTag',
            '.product-image img',
            '.vtex-product-images img',
            'img[src*="legobrasil.vtexassets.com"]',
            'img[src*="lego"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for img in elements:
                src = img.get('src') or img.get('data-src')
                if src and 'lego' in src.lower():
                    # Converter URL relativa para absoluta
                    full_url = urljoin(base_url, src)
                    if full_url not in images:
                        images.append(full_url)
        
        # Se nÃ£o encontrou imagens, usar placeholder
        if not images:
            images = ["/placeholder.svg?height=400&width=400&text=LEGO+Product"]
        
        return images[:4]  # MÃ¡ximo 4 imagens
    
    def extract_description(self, soup):
        """Retorna sempre uma descriÃ§Ã£o padrÃ£o para todos os produtos"""
        return "Construa, brinque e exiba! Este incrÃ­vel conjunto LEGO oferece uma experiÃªncia de construÃ§Ã£o envolvente com peÃ§as de alta qualidade e detalhes autÃªnticos. Perfeito para fÃ£s de todas as idades, este conjunto proporciona horas de diversÃ£o criativa e Ã© ideal para exibiÃ§Ã£o. Inclui instruÃ§Ãµes passo a passo e elementos Ãºnicos que tornam cada construÃ§Ã£o especial. Desperte sua imaginaÃ§Ã£o e crie memÃ³rias duradouras com este fantÃ¡stico conjunto LEGO."
    
    def extract_pieces(self, soup):
        """Extrai o nÃºmero de peÃ§as"""
        pieces_element = soup.select_one('.legobrasil-product-0-x-specificationsProductItemTitle')
        if pieces_element:
            pieces_text = pieces_element.get_text()
            pieces_match = re.search(r'(\d+)', pieces_text)
            if pieces_match:
                return int(pieces_match.group(1))
        
        # Fallback para busca geral no texto
        text = soup.get_text()
        patterns = [
            r'(\d+)\s*pe[Ã§c]as?',
            r'(\d+)\s*pcs?',
            r'(\d+)\s*pieces?'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 150  # Valor padrÃ£o
    
    def extract_age(self, soup):
        """Extrai a idade recomendada"""
        text = soup.get_text()
        
        # Procurar padrÃµes como "8+", "6-12", "18+ anos"
        patterns = [
            r'(\d+)\+\s*anos?',
            r'(\d+)\+',
            r'(\d+)\s*-\s*\d+\s*anos?'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return f"{match.group(1)}+"
        
        return "8+"  # Valor padrÃ£o
    
    def scrape_products(self, urls):
        """Extrai dados de mÃºltiplos produtos"""
        products = []
        
        print(f"Iniciando extraÃ§Ã£o de {len(urls)} produtos...")
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] Processando produto...")
            
            product_data = self.extract_product_data(url)
            if product_data:
                products.append(product_data)
            
            # Delay entre requisiÃ§Ãµes para nÃ£o sobrecarregar o servidor
            time.sleep(2)
        
        return products
    
    def save_to_json(self, products, filename="produtos_lego.json"):
        """Salva os produtos em arquivo JSON"""
        products_dict = {}
        for product in products:
            products_dict[product["id"]] = product
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("// Mock product data - in a real app this would come from a database\n")
            f.write("const products = ")
            json.dump(products_dict, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ {len(products)} produtos salvos em {filename}")

    def extract_item_number(self, url):
        """Extrai o nÃºmero do item da URL"""
        match = re.search(r'/(\d+)-', url)
        if match:
            return match.group(1)
        return "00000"
    
    def extract_vip_points(self, soup, price):
        """Extrai VIP points do site ou calcula baseado no preÃ§o"""
        # Procurar por VIP points no site
        vip_selectors = [
            '[data-testid="vip-points"]',
            '.vip-points',
            '.loyalty-points'
        ]
        
        for selector in vip_selectors:
            element = soup.select_one(selector)
            if element:
                vip_text = element.get_text()
                vip_match = re.search(r'(\d+)', vip_text)
                if vip_match:
                    return int(vip_match.group(1))
        
        # Se nÃ£o encontrar, calcular baseado no preÃ§o (aproximadamente 6.5% do preÃ§o)
        return int(price * 0.065)

def main():
    # URLs de exemplo - substitua pelas URLs reais
    urls = []
    
    # Tentar ler URLs do arquivo urls.txt
    try:
        with open('scripts/urls.txt', 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # Ignorar linhas vazias e comentÃ¡rios
                if line and not line.startswith('#'):
                    urls.append(line)
        print(f"âœ“ {len(urls)} URLs carregadas do arquivo urls.txt")
    except FileNotFoundError:
        print("âŒ Arquivo urls.txt nÃ£o encontrado!")
        print("Crie o arquivo scripts/urls.txt e adicione as URLs dos produtos (uma por linha)")
        return
    except Exception as e:
        print(f"âŒ Erro ao ler urls.txt: {e}")
        return
    
    if not urls:
        print("âŒ Nenhuma URL vÃ¡lida encontrada no arquivo urls.txt")
        return
    
    # Se URLs foram passadas como argumentos, usar elas ao invÃ©s do arquivo
    if len(sys.argv) > 1:
        urls = sys.argv[1:]
        print(f"âœ“ Usando {len(urls)} URLs dos argumentos da linha de comando")
    
    scraper = LegoScraper()
    products = scraper.scrape_products(urls)
    
    if products:
        scraper.save_to_json(products)
        print(f"\nğŸ‰ ExtraÃ§Ã£o concluÃ­da! {len(products)} produtos processados.")
    else:
        print("\nâŒ Nenhum produto foi extraÃ­do com sucesso.")

if __name__ == "__main__":
    main()
